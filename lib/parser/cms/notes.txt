




Notes on how to integrate my part of the code to the rest of the system.

First, add a line that says: 
In /lib/parser.js,

1. add
	require('(directory structure/CMSparser') or any other packages
2. In the last line of the file, you need to put the key value pair for the new CMSParser package


/////////////////////////////////////////////

Parser Structure

need to have something for reading in and converting file formats to one format
(use buffer)

Use a generic regular expression to get all the sections.

Parser may need to be modified, in terms of regular expressions
- make sure you can easily modify the regular expression of the parser
- by allowing it to be able to pass in parameters to modify the regular
- expression

Parser Structure

Component that converts utf coding to ascii, or  vice versa

CMS Textfile Terminology


Factor out regular expression look up, etc.

	-test parser


Section is the entire chunk of data that includes the header and body.

Header is:

-------------------------------

Insert Section Name <--- header

-------------------------------

Body is the ENTIRE data from current header to the next header.

I refer to children as the object children as the group of data within
each section body



/////////////////////////////////////////////

Todo list:

1. Convert either regular expressions or file to one format
2. Need to restructure parsing claims
	Make sure to look at line 664
	Approach 1: Modify the regular expression so that it does take in an alphanumeric in the middle of the dashes for the header,
		Modify it so that it does do take the entire claims summary header. Probably not possible.
	Approach 2: Slight modification to the regular expression so that it does take in all the data, but you need to sort claims manually
	Approach 3: Check for the source tag to understand that it is a section
	Approach 4: Fix sectionString so that it does absorb some of the dashes. 
	Approach 5: Use indexOf to find "Claims Summary", divide the string in half, only run regular expression up to that point...
3. Remove spaces within values
4. Probably don't need to process the header information on top - ask about that
5. Might need to make the parsing recursive, based on information given in last section.
6. Ask about recursion - as in 2nd level depth to the parsing. 


Todolist 6/8/14

	Questions:
		Should we change data model labels like CDA_address, CDA_name, etc?


	Does code break? 
		Remove demographics - does it break? 
		Missing sections, missing fields, etc. 
		Need feedback. 

	Need proper codes for each medical term. May need to put uncoded tag. (Look at medical dictionaries/clinical vocabularies at bottom of CCDA pad)


	Converting it to bluebutton.js model 

Work log:

6/3/14 - Started project, reviewed regular expressions and blue button, started coding
6/4/14 - Finished parsing most of the document, completed almost all of the regular expressions.
6/5/14 

Back log: 
	- Debug regular expressions, debug claims history(decide on approach), get metadata for the type of information
		- Can do this easiliy if you create a regular expression evaluator.
	- Ask about whether some blue button code can be reused, and go from there
				Answer is no.
	- From this, start working on how to convert what you have into the respective data types.
	- Refactor code for clarity. 





Plans:
	
6/5/14		

6/6/14		- Get most of the CMS object 

6/7/14 - 6/8/14 - Prepare so that you can possibly get this done by Monday.

6/9/14 - Get the code in finished version

6/10/14 - Clean up code, do testing

/////////////////////////////////////////////


Claim data not in data model
	Come up with some model for 
		High level description 
		Each claim line


		blue button

			feeed data file
			method called sense
				looks for tags for CCDA, XML,
				need function to check if it's CMS(look at the header)
					Does it have medicarehealth.gov
					Does it have generated by "Blue Button 2.0"
					etc
					etc

Scrum meeting 6/5/14 

	Needs a library/API for medical term lookups. 
		LOINC and SNOWMED 
		- Hold up on this, 
		- Normalize uncoded data into coded data
		- Investigating having a way to load libraries and vocabularies - build some API around it. 
		- Terminology server or service - maybe a project that's been done before. 
			- Think about mid to long term look up. 
		- npi? 

	Invent new sections for claims.

Testing framework:
Mocha

Superancient
